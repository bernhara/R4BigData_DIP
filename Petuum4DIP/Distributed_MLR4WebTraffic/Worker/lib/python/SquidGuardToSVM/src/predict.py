'''
MLRSGDSolver::Predict for feature: 0:0.402314 1:-0.774283 2:-1.08144 3:-0.759492 4:-0.659005 5:1.89889 6:0.51737 7:0.236722 8:-0.17063 9:0.471012 10:1 35:1 (feature dim: 54)
num_label = 0
y_vec[0] = 0
w_cache_[0] = 0:0 1:0 2:0 3:0 4:0 5:0 6:0 7:0 8:0 9:0 10:0 11:0 12:0 13:0 14:0 15:0 16:0 17:0 18:0 19:0 20:0 21:0 22:0 23:0 24:0 25:0 26:0 27:0 28:0 29:0 3\
0:0 31:0 32:0 33:0 34:0 35:0 36:0 37:0 38:0 39:0 40:0 41:0 42:0 43:0 44:0 45:0 46:0 47:0 48:0 49:0 50:0 51:0 52:0 53:0 (feature dim: 54)
num_label = 1
y_vec[1] = 0
w_cache_[1] = 0:0 1:0 2:0 3:0 4:0 5:0 6:0 7:0 8:0 9:0 10:0 11:0 12:0 13:0 14:0 15:0 16:0 17:0 18:0 19:0 20:0 21:0 22:0 23:0 24:0 25:0 26:0 27:0 28:0 29:0 3\
0:0 31:0 32:0 33:0 34:0 35:0 36:0 37:0 38:0 39:0 40:0 41:0 42:0 43:0 44:0 45:0 46:0 47:0 48:0 49:0 50:0 51:0 52:0 53:0 (feature dim: 54)
num_label = 2
y_vec[2] = 0
w_cache_[2] = 0:0 1:0 2:0 3:0 4:0 5:0 6:0 7:0 8:0 9:0 10:0 11:0 12:0 13:0 14:0 15:0 16:0 17:0 18:0 19:0 20:0 21:0 22:0 23:0 24:0 25:0 26:0 27:0 28:0 29:0 30:0 31:0 32:0 33:0 34:0 35:0 36:0 37:0 38:0 39:0 40:0 41:0 42:0 43:0 44:0 45:0 46:0 47:0 48:0 49:0 50:0 51:0 52:0 53:0 (feature dim: 54)
num_label = 3
y_vec[3] = 0
w_cache_[3] = 0:0 1:0 2:0 3:0 4:0 5:0 6:0 7:0 8:0 9:0 10:0 11:0 12:0 13:0 14:0 15:0 16:0 17:0 18:0 19:0 20:0 21:0 22:0 23:0 24:0 25:0 26:0 27:0 28:0 29:0 30:0 31:0 32:0 33:0 34:0 35:0 36:0 37:0 38:0 39:0 40:0 41:0 42:0 43:0 44:0 45:0 46:0 47:0 48:0 49:0 50:0 51:0 52:0 53:0 (feature dim: 54)
num_label = 4
y_vec[4] = 0
w_cache_[4] = 0:0 1:0 2:0 3:0 4:0 5:0 6:0 7:0 8:0 9:0 10:0 11:0 12:0 13:0 14:0 15:0 16:0 17:0 18:0 19:0 20:0 21:0 22:0 23:0 24:0 25:0 26:0 27:0 28:0 29:0 30:0 31:0 32:0 33:0 34:0 35:0 36:0 37:0 38:0 39:0 40:0 41:0 42:0 43:0 44:0 45:0 46:0 47:0 48:0 49:0 50:0 51:0 52:0 53:0 (feature dim: 54)
num_label = 5
y_vec[5] = 0
w_cache_[5] = 0:0 1:0 2:0 3:0 4:0 5:0 6:0 7:0 8:0 9:0 10:0 11:0 12:0 13:0 14:0 15:0 16:0 17:0 18:0 19:0 20:0 21:0 22:0 23:0 24:0 25:0 26:0 27:0 28:0 29:0 30:0 31:0 32:0 33:0 34:0 35:0 36:0 37:0 38:0 39:0 40:0 41:0 42:0 43:0 44:0 45:0 46:0 47:0 48:0 49:0 50:0 51:0 52:0 53:0 (feature dim: 54)
num_label = 6
y_vec[6] = 0
w_cache_[6] = 0:0 1:0 2:0 3:0 4:0 5:0 6:0 7:0 8:0 9:0 10:0 11:0 12:0 13:0 14:0 15:0 16:0 17:0 18:0 19:0 20:0 21:0 22:0 23:0 24:0 25:0 26:0 27:0 28:0 29:0 30:0 31:0 32:0 33:0 34:0 35:0 36:0 37:0 38:0 39:0 40:0 41:0 42:0 43:0 44:0 45:0 46:0 47:0 48:0 49:0 50:0 51:0 52:0 53:0 (feature dim: 54)
y_vec for all labels:
0
0
0
0
0
0
0
y_vec after SoftMax:
0.142878
0.142878
0.142878
0.142878
0.142878
0.142878
0.142878












=================================================================





MLRSGDSolver::Predict for feature: 0:-0.833488 1:-0.577696 2:-0.680908 3:-0.547777 4:-0.573242 5:-0.685041 6:0.853567 7:0.135559 8:-0.536452 9:0.105506 12:1 24:1 (feature dim: 54)
num_label = 0
y_vec[0] = 0.629833
w_cache_[0] = 0:1.59499 1:0.0631476 2:-0.0889915 3:-0.153487 4:-0.208093 5:-0.201676 6:-0.181234 7:-0.323868 8:0.0524764 9:0.166522 10:2.36047 11:0.517906 12:1.85084 13:-0.0319189 14:-0.00121719 15:-0.0899653 16:-0.00555821 17:-0.170897 18:-0.00016919 19:-0.0118653 20:-0.0848328 21:0 22:0.260727 23:0.050213 24:-0.048063 25:0.041467 26:0.472373 27:0 28:0 29:-0.0051953 30:-0.0630594 31:-0.027148 32:-0.0464251 33:0.556258 34:0 35:1.04103 36:1.36441 37:0.0174945 38:0 39:-0.219045 40:0 41:0 42:0.889486 43:0.319443 44:1.01469 45:0.338481 46:0.63607 47:-0.115169 48:0.136874 49:0 50:0 51:-0.353944 52:-0.586411 53:-0.61277 (feature dim: 54)
num_label = 1
y_vec[1] = 3.12866
w_cache_[1] = 0:-0.380177 1:-0.0790251 2:0.246036 3:0.154875 4:-0.0069198 5:0.021324 6:-0.0534926 7:0.126262 8:0.111278 9:-0.0342851 10:3.17854 11:1.28471 12:2.302 13:0.0409998 14:-0.0240537 15:-0.414734 16:-0.250074 17:0.147724 18:-0.00819496 19:0.144337 20:0.128681 21:0 22:-0.0231193 23:0.710499 24:0.818989 25:0.896977 26:0.208593 27:0 28:0 29:-0.182409 30:-0.0671285 31:0.0876529 32:0.26836 33:-0.239158 34:0 35:0.365888 36:0.304968 37:0.780093 38:0 39:-0.158742 40:0 41:0 42:1.0352 43:0.405413 44:0.111509 45:1.0859 46:1.05375 47:0.257774 48:-0.0115676 49:0 50:0 51:-0.330348 52:-0.155442 53:-0.141097 (feature dim: 54)
num_label = 2
y_vec[2] = -0.415136
w_cache_[2] = 0:-1.71244 1:0.459152 2:0.689163 3:0.317291 4:-0.0435357 5:0.0573203 6:0.220645 7:0.397034 8:-0.144031 9:-0.350279 10:-1.17688 11:-0.232823 12:-0.985932 13:0.226922 14:0.141732 15:0.629503 16:0.416923 17:0.159291 18:0.0318611 19:-0.237018 20:-0.00294772 21:0 22:-0.0796326 23:-0.418736 24:-0.21635 25:-0.206408 26:-0.343896 27:0 28:0 29:-0.0930291 30:0.243524 31:-0.0102362 32:-0.0122879 33:-0.0333315 34:0 35:-0.208474 36:-0.259244 37:-0.142264 38:0 39:-0.0331565 40:0 41:0 42:-0.374666 43:-0.202824 44:-0.135442 45:-0.326431 46:-0.362804 47:-0.0165493 48:-0.00127916 49:0 50:0 51:-0.0338746 52:-0.0299111 53:-0.0107563 (feature dim: 54)
num_label = 3
y_vec[3] = -1.54497
w_cache_[3] = 0:-0.0466189 1:-0.000358302 2:0.0478259 3:0.0204261 4:0.0322002 5:0.180398 6:0.00331621 7:0.010769 8:-0.0158899 9:0.022854 10:-1.19656 11:-0.346561 12:-1.32024 13:-0.196255 14:-0.011433 15:-0.0658692 16:-0.018107 17:-0.0697166 18:-0.00225389 19:-0.0688386 20:-0.00859879 21:0 22:-0.0225848 23:-0.169161 24:-0.0932322 25:-0.15303 26:-0.0715681 27:0 28:0 29:-0.00622917 30:-0.0195314 31:-0.00782342 32:-0.0330378 33:-0.0570331 34:0 35:-0.237272 36:-0.260573 37:-0.162302 38:0 39:-0.0322767 40:0 41:0 42:-0.402867 43:-0.186028 44:-0.149488 45:-0.313802 46:-0.243121 47:-0.019485 48:-0.008625 49:0 50:0 51:-0.0761654 52:-0.0531442 53:-0.0364183 (feature dim: 54)
num_label = 4
y_vec[4] = 0.300126
w_cache_[4] = 0:-0.328631 1:-0.362143 2:-0.609187 3:-0.244752 4:0.135651 5:-0.383107 6:0.094355 7:0.167498 8:0.0378808 9:0.0782717 10:-0.826067 11:-0.390221 12:-0.830259 13:-0.243368 14:-0.0148286 15:-0.0967773 16:-0.0258071 17:-0.0877868 18:-0.00238683 19:-0.11136 20:-0.0131614 21:0 22:-0.0590228 23:-0.187717 24:-0.177467 25:-0.235244 26:-0.0632917 27:0 28:0 29:-0.0129667 30:-0.044267 31:-0.0232645 32:-0.0508207 33:-0.075768 34:0 35:-0.277425 36:-0.354342 37:0.124534 38:0 39:0.532646 40:0 41:0 42:-0.443181 43:0.092688 44:-0.179269 45:-0.071591 46:-0.241907 47:-0.0183646 48:-0.0117062 49:0 50:0 51:-0.0728972 52:-0.0500338 53:-0.0371276 (feature dim: 54)
num_label = 5
y_vec[5] = -0.132952
w_cache_[5] = 0:-1.29528 1:-0.0411686 2:-0.0985697 3:-0.046658 4:0.324802 5:0.0793085 6:0.0905092 7:-0.00379816 8:-0.0580962 9:-0.169723 10:-1.21179 11:-0.236541 12:-0.974535 13:0.253707 14:-0.087506 15:0.0699159 16:-0.110611 17:0.0666788 18:-0.0181738 19:0.302725 20:-0.00581305 21:0 22:-0.0679248 23:0.105934 24:-0.203952 25:-0.204449 26:-0.111741 27:0 28:0 29:0.301762 30:-0.0383105 31:-0.0158527 32:-0.0206817 33:-0.0471481 34:0 35:-0.204288 36:-0.251632 37:-0.158911 38:0 39:-0.0494154 40:0 41:0 42:-0.384441 43:-0.18883 44:-0.142291 45:-0.304191 46:-0.272578 47:-0.022302 48:-0.00330013 49:0 50:0 51:-0.0443338 52:-0.0402051 53:-0.0172982 (feature dim: 54)
num_label = 6
y_vec[6] = -1.98521
w_cache_[6] = 0:2.16833 1:-0.0393002 2:-0.185363 3:-0.0472567 4:-0.233023 5:0.248949 6:-0.173406 7:-0.374316 8:0.0153445 9:0.287082 10:-1.14133 11:-0.598902 12:-0.0579152 13:-0.0517254 14:-0.0027963 15:-0.0327046 16:-0.00695254 17:-0.0459845 18:-0.000720887 19:-0.0184197 20:-0.0133972 21:0 22:-0.00878713 23:-0.0930527 24:-0.0811634 25:-0.141207 26:-0.0913071 27:0 28:0 29:-0.00200141 30:-0.0114501 31:-0.00343703 32:-0.105327 33:-0.104378 34:0 35:-0.481891 36:-0.546708 37:-0.460198 38:0 39:-0.0402958 40:0 41:0 42:-0.324178 43:-0.241852 44:-0.521704 45:-0.411901 46:-0.571973 47:-0.066088 48:-0.100456 49:0 50:0 51:0.910727 52:0.914547 53:0.85518 (feature dim: 54)
y_vec for all labels:
0.629833
3.12866
-0.415136
-1.54497
0.300126
-0.132952
-1.98521
y_vec after SoftMax:
0.0671654
0.817307
0.0236235
0.00763237
0.0483034
0.0313251
0.00491428






#============================================================================

const float kCutoff = 1e-15;

void Softmax(std::vector<float>* vec) {
  CHECK_NOTNULL(vec);
  // TODO(wdai): Figure out why this is necessary. Doubt it is.
    for (int i = 0; i < vec->size(); ++i) {
        if (std::abs((*vec)[i]) < kCutoff) {
            (*vec)[i] = kCutoff;
        }
    }
    double lsum = LogSumVec(*vec);
    for (int i = 0; i < vec->size(); ++i) {
        (*vec)[i] = fastexp((*vec)[i] - lsum);
        (*vec)[i] = (*vec)[i] > 1 ? 1. : (*vec)[i];
  }
}

float DenseDenseFeatureDotProduct(const AbstractFeature<float>& f1, const AbstractFeature<float>& f2) {
  CHECK_EQ(f1.GetFeatureDim(), f2.GetFeatureDim());
  auto f1_dense_ptr = static_cast<const DenseFeature<float>*>(&f1);
  auto f2_dense_ptr = static_cast<const DenseFeature<float>*>(&f2);
  const std::vector<float>& v1 = f1_dense_ptr->GetVector();
  const std::vector<float>& v2 = f2_dense_ptr->GetVector();
  Eigen::Map<const Eigen::VectorXf> e1(v1.data(), v1.size());
  Eigen::Map<const Eigen::VectorXf> e2(v2.data(), v2.size());
  return e1.dot(e2);
}

float SparseDenseFeatureDotProduct(const AbstractFeature<float>& f1,
    const AbstractFeature<float>& f2) {
  CHECK_EQ(f1.GetFeatureDim(), f2.GetFeatureDim());
  float sum = 0.;
  for (int i = 0; i < f1.GetNumEntries(); ++i) {
    int32_t f1_fid = f1.GetFeatureId(i);
    sum += f1.GetFeatureVal(i) * f2[f1_fid];
  }
  return sum;
}

float DenseSparseFeatureDotProduct(const AbstractFeature<float>& f1,
    const AbstractFeature<float>& f2) {
  return SparseDenseFeatureDotProduct(f2, f1);
}

float SparseSparseFeatureDotProduct(const AbstractFeature<float>& f1,
    const AbstractFeature<float>& f2) {
  CHECK_EQ(f1.GetFeatureDim(), f2.GetFeatureDim());
  int j = 0;
  float sum = 0.;
  int f2_num_entries = f2.GetNumEntries();
  for (int i = 0; i < f1.GetNumEntries() && j < f2_num_entries; ++i) {
    int32_t f1_fid = f1.GetFeatureId(i);
    while (f2.GetFeatureId(j) < f1_fid && j < f2_num_entries) {
      ++j;
    }
    if (f1_fid == f2.GetFeatureId(j)) {
      sum += f1.GetFeatureVal(i) * f2.GetFeatureVal(j);
    }
  }
  return sum;
}



'''

# XXXXXXXXXXXXXXXXXXXXXXX

import sys

import argparse

import logging
logging.basicConfig (level=logging.WARNING)

import unittest

_feature_one_based = False
_label_one_based = False

def read_peetuum_mlr_weight_file (weight_file_name):
    
    weight_dict ={}
    
    with open (weight_file_name, 'r') as weight_file:
        
        for _ in range(2):
            line = weight_file.readline()
            line_elements_list = line.split(':')
                 
            key = line_elements_list[0].strip()
            value = line_elements_list[1]     
            weight_dict[key] = int(value)
        
        num_labels = weight_dict["num_labels"]
        for label_number in range (num_labels):
            line = weight_file.readline()
            vector_dict = libsvm_feature_vector_to_dict (line, feature_one_based = False)
            
            weight_dict[label_number] = vector_dict
                 
    return (weight_dict)

def read_libsvm_file (libsvm_file_name, label_one_based, feature_one_based):
    
    file_content_list =[]
        
    with open (libsvm_file_name, 'r') as libsvm_file:
        
        while True:
            
            line = libsvm_file.readline()
            if not line:
                break
            
            content = labeled_libsvm_vector_to_label_and_dict (line, label_one_based, feature_one_based)
            file_content_list.append (content)
        
    return (file_content_list)


def libsvm_feature_vector_to_dict (libsvm_feature_line, feature_one_based):

    weighted_attribute_list = libsvm_feature_line.split()
    
    if feature_one_based:
        attribute_weigth_dict ={int(k)-1:float(v) for k,v in (x.split(':') for x in weighted_attribute_list)}
    else:
        attribute_weigth_dict ={int(k):float(v) for k,v in (x.split(':') for x in weighted_attribute_list)}    
    
    return (attribute_weigth_dict)

def labeled_libsvm_vector_to_label_and_dict (labeled_libsvm_line, label_one_based, feature_one_based):
    
    label_vector_pair = labeled_libsvm_line.split(maxsplit = 1)
    
    if label_one_based:
        label = int(label_vector_pair[0]) - 1
    else:
        label = int(label_vector_pair[0])

    libsvm_feature_string =  label_vector_pair[1]
    attribute_weigth_dict = libsvm_feature_vector_to_dict (libsvm_feature_string, feature_one_based)
   
    return (label, attribute_weigth_dict)

def libsvm_data_scalar_vector_product (v1, v2):
    
    product = 0.0
    for v1_key, v1_value in v1.items():
        v2_value = v2.get(v1_key)
        if v2_value:
            factor = v1_value * v2_value
            product += factor
       
    return (product)

#============================================================================
'''
const float kCutoff = 1e-15;

void Softmax(std::vector<float>* vec) {
  CHECK_NOTNULL(vec);
  // TODO(wdai): Figure out why this is necessary. Doubt it is.
    for (int i = 0; i < vec->size(); ++i) {
        if (std::abs((*vec)[i]) < kCutoff) {
            (*vec)[i] = kCutoff;
        }
    }
    double lsum = LogSumVec(*vec);
    for (int i = 0; i < vec->size(); ++i) {
        (*vec)[i] = fastexp((*vec)[i] - lsum);
        (*vec)[i] = (*vec)[i] > 1 ? 1. : (*vec)[i];
  }
}
'''

import math
import functools

def LogSum(log_a, log_b):
    if (log_a < log_b):
        r = log_b + fastlog(1 + fastexp(log_a - log_b))
    else:   
        r = log_a + fastlog(1 + fastexp(log_b-log_a))
        
    return r

def LogSumVec(vec_as_list):
    
    # FIXME: remove bad code
    #!! sum = functools.reduce(lambda x, y: x+y, vec_as_list)

    sum = vec_as_list[0]
    for i in range(1, len(vec_as_list)):
        sum = LogSum(sum, vec_as_list[i])
        
    return sum

'''
float LogSum(float log_a, float log_b) {
  return (log_a < log_b) ? log_b + fastlog(1 + fastexp(log_a - log_b)) :
    log_a + fastlog(1 + fastexp(log_b-log_a));
}

float LogSumVec(const std::vector<float>& logvec) {
        float sum = 0.;
        sum = logvec[0];
        for (int i = 1; i < logvec.size(); ++i) {
                sum = LogSum(sum, logvec[i]);
        }
        return sum;
}
'''



def fastpow2 (p):
    r = math.pow(2.0, p)
    return r


def fastexp (p):
  return fastpow2 (1.442695040 * p)


'''
fastlog2 (float x)
{
  union { float f; uint32_t i; } vx = { x };
  union { uint32_t i; float f; } mx = { (vx.i & 0x007FFFFF) | 0x3f000000 };
  float y = vx.i;
  y *= 1.1920928955078125e-7f;

  return y - 124.22551499f
           - 1.498030302f * mx.f
           - 1.72587999f / (0.3520887068f + mx.f);
}

static inline float
fastlog (float x)
{
  return 0.69314718f * fastlog2 (x);
}
'''

def fastlog2 (x):
    r = math.log(x, 2)
    return (r)

def fastlog (x):
    return 0.69314718 * fastlog2 (x)

def Softmax(vec_as_list):
    
    lsum = LogSumVec(vec_as_list)
    softmax_vector_as_list = [0.0 for _ in vec_as_list]
    for i, val_for_i in enumerate(vec_as_list):
        softmax_vector_item =  fastexp(val_for_i - lsum)
        if softmax_vector_item > 1:
            softmax_vector_item = 1.0
        softmax_vector_as_list[i] = softmax_vector_item
    
    return softmax_vector_as_list


'''
std::vector<float> MLRSGDSolver::Predict(const petuum::ml::AbstractFeature<float>& feature) const
{
    std::vector<float> y_vec(num_labels_);

#ifdef RAPH
    LOG(INFO) << "RAPH: " << "MLRSGDSolver::Predict for feature: " << feature.ToString();
#endif

    for (int i = 0; i < num_labels_; ++i) {
      y_vec[i] = FeatureDotProductFun_(feature, w_cache_[i]);
#ifdef RAPH
      LOG(INFO) << "RAPH: "<< "num_label = " << i ;
      LOG(INFO) << "RAPH: "<< "y_vec[" << i << "] = " << y_vec[i];
      LOG(INFO) << "RAPH: "<< "w_cache_[" << i << "] = " << w_cache_[i].ToString();
#endif
    }
    LOG(INFO) << "y_vec for all labels: ";
    for (const auto i: y_vec) {
      LOG(INFO) << "RAPH: " << i;
    }
    petuum::ml::Softmax(&y_vec);
    LOG(INFO) << "y_vec after SoftMax: ";
    for (const auto i: y_vec) {
      LOG(INFO) << "RAPH: " << i;
    }

    return y_vec;
  }

'''



def SparseDenseFeatureDotProduct(f1_feature_dict, f2_feature_dict):
    
    sum = 0.0
    for f1_key, f1_value in f1_feature_dict.items():
        f2_value = f2_feature_dict.get(f1_key)
        if f2_value:
            factor = f1_value * f2_value
            sum += factor
       
    return (sum)


def Predict (attribute_dict, petuum_mlr_computed_label_weights):
    pass
    
    
    

def predict_label_index (attribute_dict, petuum_mlr_computed_label_weights):
    
    nb_labels = petuum_mlr_computed_label_weights["num_labels"]
      
    computed_factor_dict = {}
    for label_index in range (nb_labels):
            
            computed_weights_for_label_index = petuum_mlr_computed_label_weights[label_index]
            computed_factor = libsvm_data_scalar_vector_product (attribute_dict, computed_weights_for_label_index)
            computed_factor = softmax (attribute_dict, computed_weights_for_label_index)
            computed_factor_dict[label_index] = computed_factor
            
    # trace 
    for label_index in range (nb_labels):
        print ('\t\tChecked weight matrix for label index: {} | Resulting label factor for the checked sample : {}'.format(label_index, computed_factor_dict[label_index]))
        
    # predict label by getting the label index having the greatest factor
    geatest_factor = -sys.float_info.max
    highest_label_index = None
    for label_index in range (nb_labels):
        if computed_factor_dict[label_index] > geatest_factor:
            geatest_factor = computed_factor_dict[label_index]
            highest_label_index = label_index
            
    predicted_label_index = highest_label_index
    
    print ('\t\tPredicted label index {} with score : {}'.format(predicted_label_index, geatest_factor))
    
    return (predicted_label_index)

def main():
    
    #
    # manage program options
    #
    
    # TODO:
    # add *one_based params
    
    parser = argparse.ArgumentParser(description='Apply the prediction function based on computed weights and check if predicition is OK with already known classification.')
    parser.add_argument("-w", "--weitghFile", metavar='<the weight file generated by MLR learning>', type=str, dest="weitghFile", required=True, 
                        help='The weight file generated by MLR learning.')
    parser.add_argument("-p", "--libSVMFile", metavar='<libsvm for Petuum MLR>', type=str, dest="libSVMFile", required=True,
                        help='''The "LIB SVM" formated file, containing the classified content.
    The additional file with "<libSVMFile>.meta" suffix is generated, which contains the information required by Petuum's MLR algorithm.
    These 2 files can be used as input to Petuum's MRL''') 
    parser.add_argument("--featureOneBased", action='store_true', dest="featureOneBased", 
                        help='If true, feature indexes start at "1", "0" else (default is false => first feature index is "0"')
    parser.add_argument("--labelOneBased", action='store_true', dest="labelOneBased",
                        help='If true, labels indexes start at "1", "0" else (default is false => first label index is "0"')
    parser.add_argument("-d", "--debug", action='store_true', dest="debug")       

    args = parser.parse_args()
    
    if args.featureOneBased:
        _feature_one_based = True
    else:
        _feature_one_based = False
        
    if args.labelOneBased:
        _label_one_based = True
    else:
        _label_one_based = False
        
    if args.debug:
        logging.getLogger().setLevel (logging.DEBUG)
    
    
    #
    # validate results
    
    petuum_mlr_computed_label_weights = read_peetuum_mlr_weight_file (args.weitghFile)
    test_sample_list = read_libsvm_file (args.libSVMFile, label_one_based = False, feature_one_based = False)
    
    test_sample_line_number = 1
    for test_sample in test_sample_list:
        sample_label_index, sample_attribute_dict = test_sample
        
        print ('Checking test sample line # {} which has label index {}'.format(test_sample_line_number, sample_label_index))
        
        predicted_label_index = predict_label_index (attribute_dict = sample_attribute_dict,
                                                     petuum_mlr_computed_label_weights = petuum_mlr_computed_label_weights)
        
        if predicted_label_index == sample_label_index:
            print ('\tMATCHED label prediction')
        else:
            print ('\t=== MISSED !!! label prediction')

        
        test_sample_line_number += 1

#     label_index_to_check = 0
#     train_line_to_check = train_line_list[0]
    
#     label_weight_attribute_dict = petuum_mlr_computed_label_weights[label_index_to_check]
#     train_label, train_attribute_dict = labeled_libsvm_vector_to_label_and_dict (train_line_to_check, label_one_based = True, feature_one_based = True)
#     
#     computed_factor = libsvm_data_scalar_vector_product (train_attribute_dict, label_weight_attribute_dict)
#     
#     print (computed_factor)


l1 = [1.2659, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0154804, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.00559692, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0154895, -0.0253445, -0.0253445, -0.0253445, -0.0154732, 0.242097, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, 0.15328, -0.0253445, -0.0253445, -0.0253445, 0.0345414, -0.0253445, -0.0253445, 0.0638459, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, 0.113523, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445, -0.0253445]
l2 = [0.0426195, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0118334, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0119509, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0118333, 0.0117173, 0.0117173, 0.0117173, 0.0118334, 0.0153098, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0140082, 0.0117173, 0.0117173, 0.0117173, 0.0124401, 0.0117173, 0.0117173, 0.01281, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0134622, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173, 0.0117173]



class moduleTestCases (unittest.TestCase):
    
    def floatTruncatedString (self, f, n_digits = 14):
        
        # float formating function rounds the value
        # use string manipulation instead
        
        f_as_string = str(float(f))
        dot_position = f_as_string.index('.')
        (natural_part, decimal_part) = f_as_string.split(sep='.')
        
        str_containing_only_zeros = ''.zfill(20) # assert that n_digits is < than 20
        decimal_part += str_containing_only_zeros
        # truncate decimal_part
        decimal_part = decimal_part[:n_digits]

        rounded_string_for_f = natural_part + '.' + decimal_part
        return rounded_string_for_f
    
     
    def test_LogSumVec (self):
        tested_precision = 3
        
        sample = [2.32699, 2.70564, 0.70979, -1.65623, -1.32633, -0.59962, -2.18235]
        sample_LogSumVec = 3.34482
        
        sum = LogSumVec(sample)
        diff = (sum - sample_LogSumVec)
        
        diff_as_string = self.floatTruncatedString (diff, tested_precision)
        zero_as_string = self.floatTruncatedString (0.0, tested_precision)
        self.assertEqual(diff_as_string, zero_as_string, 'non zero difference with sample')
         
    def test_Softmax (self):
        tested_precision = 3
        
        sample = [0.629833, 3.12866, -0.415136, -1.54497, 0.300126, -0.132952, -1.98521]
        ref_result = [0.0671654, 0.817307, 0.0236235, 0.00763237, 0.0483034, 0.0313251, 0.00491428]
        
        computed_result = Softmax(sample)    
        for r,c in zip(ref_result, computed_result):
            self.assertEqual(self.floatTruncatedString(r, tested_precision), self.floatTruncatedString(c, tested_precision), 'Softmax did not compute equivalent values')
            
    def test_SparseDenseFeatureDotProduct(self):
        tested_precision = 4
        
        v1_sample = {0:-0.722766, 1:1.6205, 2:0.12016, 3:-0.0631851, 4:-0.761921, 5:-0.922334, 6:-0.97684, 7:-0.269095, 8:0.743925, 9:-0.654204, 12:1, 44:1}
        v2_sample = {0:1.59727, 1:0.0593455, 2:-0.0893879, 3:-0.151879, 4:-0.210145, 5:-0.197497, 6:-0.173927, 7:-0.326345, 8:0.0456448, 9:0.166405, 10:2.36239, 11:0.521288, 12:1.84793, 13:-0.031775, 14:-0.00115621, 15:-0.0904455, 16:-0.00524351, 17:-0.171253, 18:-0.00025415, 19:-0.0118456, 20:-0.0842596, 21:0, 22:0.260991, 23:0.0506531, 24:-0.0482705, 25:0.0425302, 26:0.474681, 27:0, 28:0, 29:-0.00536855, 30:-0.0630568, 31:-0.0276387, 32:-0.046064, 33:0.556238, 34:0, 35:1.04189, 36:1.36368, 37:0.0144527, 38:0, 39:-0.220127, 40:0, 41:0, 42:0.887994, 43:0.320543, 44:1.0134, 45:0.339152, 46:0.635149, 47:-0.114862, 48:0.137085, 49:0, 50:0, 51:-0.352742, 52:-0.585151, 53:-0.610863}
        ref_result = 2.32699
        
        computed_result = SparseDenseFeatureDotProduct(v1_sample, v2_sample)
        self.assertEqual(self.floatTruncatedString (computed_result, tested_precision),
                         self.floatTruncatedString (ref_result, tested_precision))
        # commutative computation
        computed_result = SparseDenseFeatureDotProduct(v2_sample, v1_sample)
        self.assertEqual(self.floatTruncatedString (computed_result, tested_precision),
                         self.floatTruncatedString (ref_result, tested_precision))        
        
  



if __name__ == '__main__':
    
    softmax (l1, l2)
    main()